{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Predicting Diabetes in Pima Indian Women</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pima Indian Diabetes Project](diabetes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to predict whether or not a patient has diabetes using the popular public dataset regarding Pima Indian Women. All of the women are of Pima Indian heritage and at least 21 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 9 diagnostic predictor variables and a binary target variable \"diabetes\".  The features include the number of pregnancies each woman has had, glucose concentration, diastolic blood pressure, triceps skinfold thickness, insulin level, body mass index (BMI), a diabetes pedigree function (which relates the familial diabetes history of the subject), age, and another skinfold measurement. There are 768 records with 268 women who have diabetes and 500 who do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pima_diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_plot = sns.pairplot(df, hue='diabetes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visually inspecting the data to note any significant patterns or correlations, it is clear that \"thickenss\" and \"skin\" are highly correlated and appear to be duplicate measures so I dropped \"skin\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop a feature\n",
    "df = df.drop(['skin'], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the data was mostly clean, I noted that several features were encoded with a \"0\" where there should have been missing values so I replaced those \"missing values\" with null values (NaN) to better reflect the collected data. I also converted the target \"diabetes\" variable from the categorical True/False to a binary 1/0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Transform the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Missing Data\n",
    "\n",
    "# Impute missing values\n",
    "# **********  only transform to np.nan if you are going to impute, scikit-learn will give an error if nans are present\n",
    "mask = df[['glucose_conc', 'diastolic_bp', 'thickness', 'insulin','bmi', 'diab_pred']] == 0\n",
    "df[mask] = np.nan\n",
    " \n",
    "# OR\n",
    "\n",
    "\n",
    "# Drop rows that contain NANs\n",
    "# df.dropna(axis=0, how='any')    # returns a new DataFrame dropping the rows containing 'any' nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I imputed the missing data as follows...\n",
    "\n",
    "OR\n",
    "\n",
    "I dropped the rows with missing values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform features\n",
    "df['diabetes'] = df['diabetes'].map({False:0, True: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do one-hot encoding as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example:\n",
    "one_hot_df = pd.get_dummies(df, columns=[\"name_of_column_to_encode\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive the cleaned and transforned DataFrame (if desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_transformed_df.to_csv(\"new_filename.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I visually explored the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "fig, ax = plt.subplots(1, 2, figsize=(22,4))\n",
    "ax[0].hist(df.loc[df['diabetes'] == True, \"age\"], color = 'crimson', edgecolor='black')\n",
    "ax[0].set_title('Diabetes ', fontsize = 18) \n",
    "ax[0].set_xlabel('Age', fontsize = 18)\n",
    "\n",
    "ax[0].set_ylim(0,250)\n",
    "ax[1].hist(df.loc[df['diabetes'] == False, \"age\"], color = 'chartreuse', edgecolor='black')\n",
    "ax[1].set_title('No Diabetes ', fontsize = 18)\n",
    "ax[1].set_xlabel('Age', fontsize = 18)\n",
    "\n",
    "\n",
    "# Density Plot\n",
    "fig, ax = plt.subplots(figsize=(22,4))\n",
    "ax = sns.kdeplot(df.loc[df['diabetes'] == True, \"age\"], shade=True, color=sns.xkcd_rgb[\"red\"], label=\"Diabetes\")\n",
    "ax = sns.kdeplot(df.loc[df['diabetes'] == False, \"age\"], shade=True, color=sns.xkcd_rgb[\"green\"], label=\"No Diabetes\")\n",
    "ax.set_title('Age Distribution by Diabetes or No Diabetes', fontsize = 18)\n",
    "ax.set_ylabel('Density', fontsize = 18)\n",
    "ax.set_xlabel('Age', fontsize = 18)\n",
    "plt.legend(fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age appears to be related to having diabetes and thus will perhaps be a good predictor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = df.loc[df['diabetes'] == 1, 'num_preg': 'age'].values\n",
    "no_diabetes = df.loc[df['diabetes'] == 0, 'num_preg':'age'].values\n",
    "diabetes_means = np.mean(diabetes, axis=0)\n",
    "no_diabetes_means = np.mean(no_diabetes, axis=0)\n",
    "\n",
    "features = ['num_preg', 'glucose_conc', 'diastolic_bp', 'thickness', 'insulin','bmi', 'diab_pred', 'age']\n",
    "num_features = len(features)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18, 9))\n",
    "index = np.arange(num_features)\n",
    "bar_width = 0.35\n",
    "opacity = 0.6\n",
    "\n",
    "rects1 = ax.bar(index, no_diabetes_means, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='lime',\n",
    "                 label='No Diabetes')\n",
    "\n",
    "rects2 = ax.bar(index + bar_width, diabetes_means, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='crimson',\n",
    "                 label='Diabetes')\n",
    "\n",
    "ax.set_xlabel('Features', fontsize=18)\n",
    "ax.set_ylabel(\"Mean\", fontsize=18)\n",
    "ax.set_title('Mean of Each Feature', fontsize=18)\n",
    "#set_tick_labels(ax)\n",
    "plt.setp(ax, xticks=[i+bar_width for i in index],xticklabels=features)\n",
    "ax.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(NOTE: the missing values input as 0 were calculated in these means. The imputed values or dataframe with dropped rows may look very different.)\n",
    "\n",
    "Diabetics appear to have higher values on every feature. Number of pregnancies is confounded by age - the older you are the more chance that you've had to have more children...  The greatest differences appear to be between glucose concentrations and insulin levels.\n",
    "\n",
    "Discuss whatever you found with data exploration..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place the features and target variable in X and y, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now ready to start doing some data modeling. I first split the data into training and test sets. I then standardized the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Standardize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into test/train  using All features\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3, stratify=y)\n",
    "\n",
    "# standardize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(C = .1)\n",
    "\n",
    "# k-Nearest Neighbor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear', C=.1)\n",
    "\n",
    "# Random Forest (ensemble of Decision Trees)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "\n",
    "# Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Feature Engineering\n",
    "\n",
    "**Random Forest Classifier** can identify the importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "feature_importance = rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame to display the best features\n",
    "# feature_importance array needs to be reshaped; it is a one-imensional array of shape (8,), \n",
    "    # it needs to be a two-dimensioanl array of shaped (1,8)\n",
    "best_features = pd.DataFrame(feature_importance.reshape(1,-1), columns=df.columns[:-1], index = [\"importance\"])\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort using the index positions\n",
    "feature_importance.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the order to that the sort is in descending order\n",
    "pd.DataFrame(feature_importance[feature_importance.argsort()[::-1]].reshape(1, -1), columns=df.columns[:-1], index = [\"importance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your feature selection or feature engineering..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation (for model selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = [log_reg, knn, svm, rf, nn]\n",
    "\n",
    "model_scores = []\n",
    "for clf in classifiers:\n",
    "    model_scores.append(cross_val_score(clf, X_train_std, y_train, scoring='accuracy', cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame(model_scores, columns=[1,2,3,4,5,6,7,8,9,10],\n",
    "                         index=[\"LR\", \"KNN\", \"SVM\", \"Forest\", \"Neural Network\"])\n",
    "models_df[\"Mean\"] = models_df.mean(axis=1)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss what the above values mean..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot (for model selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# BOXPLOT comparing models and comparing SVM using different feature subsets\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(18, 8))\n",
    "# rectangular box plot\n",
    "bplot_models = axes.boxplot(model_scores, vert=True, patch_artist=True)\n",
    "\n",
    "# fill with colors - Models\n",
    "colors_d = [\"lightgreen\", \"lightyellow\", \"lime\", \"yellow\", \"yellowgreen\"]\n",
    "for patch, color in zip(bplot_models['boxes'], colors_d):\n",
    "    patch.set_facecolor(color)\n",
    "    \n",
    "    # adding axes labels\n",
    "axes.yaxis.grid(True)\n",
    "axes.set_xticks([y+1 for y in range(len(model_scores))])\n",
    "axes.set_xlabel('Classification Models', fontsize=18)\n",
    "axes.set_ylabel('Accuracy', fontsize=18)\n",
    "\n",
    "axes.set_title('Classification Accuracy', fontsize = 18)\n",
    "\n",
    "\n",
    "# add x-tick labels\n",
    "plt.setp(axes, xticks=[y+1 for y in range(len(model_scores))],xticklabels=['LR', 'KNN', 'SVM', 'RF', 'NN'])\n",
    "\n",
    "# increase tick size\n",
    "y_ticks = axes.get_yticklabels()\n",
    "x_ticks = axes.get_xticklabels()\n",
    "\n",
    "for x in x_ticks: \n",
    "    x.set_fontsize(18)       \n",
    "for y in y_ticks:\n",
    "    y.set_fontsize(18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the box plot..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the accuracy of the Training set versus the Test set using different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C = .1)\n",
    "log_reg.fit(X_train_std, y_train)\n",
    "train_score = log_reg.score(X_train_std, y_train)\n",
    "test_score = log_reg.score(X_test_std, y_test)\n",
    "print(\"Train score: {} \\nTest score: {}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear', C=.1)\n",
    "svm.fit(X_train_std, y_train)\n",
    "train_score = svm.score(X_train_std, y_train)\n",
    "test_score = svm.score(X_test_std, y_test)\n",
    "print(\"Train score: {} \\nTest score: {}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf.fit(X_train_std, y_train)\n",
    "train_score = rf.score(X_train_std, y_train)\n",
    "test_score = rf.score(X_test_std, y_test)\n",
    "print(\"Train score: {} \\nTest score: {}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Cross-Validation: Grid Search (for hyperparameter tuning) within cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_range = [0.0001, 0.001, .005, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "classifiers = [{\"clf\":LogisticRegression(random_state=0), \"param_grid\":[{'C': param_range}]}, \n",
    "               {\"clf\":SVC(random_state=0), \"param_grid\":[{'C': param_range, 'gamma': param_range, 'kernel': ['linear','rbf']}]}, \n",
    "               {\"clf\":RandomForestClassifier(random_state=0), \"param_grid\":[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None], \n",
    "                                                                             'max_features': [None, 'auto'],\n",
    "                                                                            'n_estimators': [10, 100, 1000]}]}\n",
    "              ]\n",
    "\n",
    "# 10x3 Nested Cross Validation  (algorithm comparison)\n",
    "model_scores = []\n",
    "for classifier in classifiers:\n",
    "    # Inner Cross Validation, searches for the best parameters\n",
    "    gs = GridSearchCV(estimator=classifier[\"clf\"], param_grid=classifier[\"param_grid\"], scoring='accuracy', cv=3)\n",
    "    # Outer Cross Validation, evaluates the model\n",
    "    model_scores.append(cross_val_score(gs, X_train_std, y_train, scoring='accuracy', cv=10))\n",
    "\n",
    "models_df = pd.DataFrame(model_scores, columns=[1,2,3,4,5,6,7,8,9,10],\n",
    "                         index=[\"LR\", \"SVM\", \"Forest\"])\n",
    "models_df[\"Mean\"] = models_df.mean(axis=1)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search (Logistic Regression) comparing Test with Training set accuracy\n",
    "\n",
    "gs = GridSearchCV(estimator=log_reg, param_grid=[{'C': param_range}], scoring='accuracy', cv=3)\n",
    "gs.fit(X_train_std, y_train)\n",
    "train_score = gs.score(X_train_std, y_train)\n",
    "test_score = gs.score(X_test_std, y_test)\n",
    "print(\"Train score: {} \\nTest score: {}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search (Support Vector Machine) comparing Test with Training set accuracy\n",
    "\n",
    "gs = GridSearchCV(estimator=svm, param_grid=[{'C': param_range, 'gamma': param_range, 'kernel': ['linear','rbf']}], scoring='accuracy', cv=3, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "train_score = gs.score(X_train, y_train)\n",
    "test_score = gs.score(X_test, y_test)\n",
    "print(\"Train score: {} \\nTest score: {}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search (Random Forest) comparing Test with Training set accuracy\n",
    "\n",
    "gs = GridSearchCV(estimator=rf, param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None], \n",
    "                                                                             'max_features': [None, 'auto'],\n",
    "                                                                            'n_estimators': [10, 100, 1000]}], scoring='accuracy', cv=3, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "train_score = gs.score(X_train, y_train)\n",
    "test_score = gs.score(X_test, y_test)\n",
    "print(\"Train score: {} \\nTest score: {}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix (for model performance metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "\n",
    "# Precision, Recall, and F1 scores\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "precision = precision_score(y_true=y_test, y_pred=y_pred)\n",
    "recall = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "f1 = f1_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "#print('Precision: {:.3f}, Recall: {:.3f}, F1: {:.3f}'.format(precision, recall, f1))\n",
    "print(classification_report(y_test, y_pred, target_names=[\"No Diabetes\", \"Diabetes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the balance of the target variable\n",
    "    # you can comment on your choice of performance measure given the balance of the target variable\n",
    "print(df.groupby('diabetes').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret and comment on the confusion matrix and the various scores (accuracy, precision and recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is anything that you did that was noteworthy or if there was anything that you might have done differently etc. , discus it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the problem, what you were able to discover from the data, and talk about your model and its ability to accurately classify the data (i.e., accuracy, precision, and/or recall)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
